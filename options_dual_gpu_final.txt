# =============================================================================
# 双GPU配置 - 大规模拓扑优化
# 使用MPI分布式计算，每个GPU处理一半数据
# =============================================================================

# ========================== 硬件映射 ========================================
-dm_mat_type aijcusparse
-dm_vec_type cuda

# ========================== 求解器配置 ======================================
-ksp_type cg
-ksp_rtol 1e-5
-ksp_atol 1e-12
-ksp_max_it 200

# ========================== 预条件器：GMG ===================================
-pc_type mg
-pc_mg_type multiplicative
-pc_mg_cycle_type v

# ========================== 细网格光滑器（GPU）==============================
-mg_levels_ksp_type chebyshev
-mg_levels_ksp_max_it 2
-mg_levels_ksp_chebyshev_esteig 0,0.1,0,1.1
-mg_levels_pc_type jacobi

# ========================== 粗网格求解器（GPU加速）========================
# 注意：在MPI环境下，粗网格矩阵是分布式的（mpiaijcusparse）
# cuSPARSE不支持MPI矩阵，所以使用迭代求解器 + GPU预条件
-mg_coarse_ksp_type cg
-mg_coarse_ksp_max_it 100
-mg_coarse_ksp_rtol 1e-8
-mg_coarse_pc_type jacobi
-mg_coarse_dm_mat_type aijcusparse
-mg_coarse_dm_vec_type cuda

# ========================== MPI 设置 ========================================
-use_gpu_aware_mpi 0

# ========================== 性能监控 ========================================
-log_view
-ksp_monitor_short
-ksp_converged_reason

# =============================================================================
# 使用方法：
# mpirun -np 2 ./topopt -options_file options_dual_gpu_final.txt -nx 257 -ny 129 -nz 129 -nlvls 4 -filter 2 -rmin 1.5 -volfrac 0.3 -maxiter 20
# =============================================================================
